//
// Licensed to the Apache Software Foundation (ASF) under one or more
// contributor license agreements.  See the NOTICE file distributed with
// this work for additional information regarding copyright ownership.
// The ASF licenses this file to You under the Apache License, Version 2.0
// (the "License"); you may not use this file except in compliance with
// the License.  You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.
//
Apache NiFi Overview
====================
Apache NiFi Team <dev@nifi.incubator.apache.org>
:homepage: http://nifi.incubator.apache.org

What is Apache NiFi?
--------------------
Put simply NiFi was built to automate the flow of data between systems.  While
the term 'dataflow' is used in a variety of contexts we'll use it here 
to mean the automated and managed flow of information between systems.  This 
problem space has been around ever since enterprises had more than one system 
where some of the systems created data and some of the systems consumed data.
The problems and solution patterns that emerged have been discussed and 
articulated extensively.  A comprehensive and readily consumed form is found in
the _Enterprise Integration Patterns_ <<eip>>.

Over the years dataflow has been one of those necessary evils in an 
architecture.  Now though there are a number of active and rapidly evolving 
movements making dataflow a lot more interesting and a lot more vital to the 
success of a given enterprise.  These include things like; Service Oriented 
Architecture <<soa>>, the rise of the API <<api>><<api2>>, Internet of Things <<iot>>,
and Big Data <<bigdata>>.  In addition, the level of rigor necessary for 
compliance, privacy, and security is constantly on the rise.  Even still with 
all of these new concepts coming about the patterns and needs of dataflow is 
still largely the same.  The primary differences then are the scope of
complexity, the rate of change necessary to adapt, and that at scale  
the edge case becomes common occurrence.  NiFi is built to help tackle these 
modern dataflow challenges.

The core concepts of NiFi
-------------------------

NiFi's fundamental design concepts closely relate to the main ideas of Flow Based
Programming <<fbp>>.  Here are some of 
the main NiFi concepts and how they map to FBP:
[grid="rows"]
[options="header",cols="3,3,10"]
|===========================
| NiFi Term | FBP Term| Description

| FlowFile | Information Packet | 
A FlowFile represents the objects moving through the system and for each one NiFi
keeps track of a Map of key/value pair attribute strings and its associated 
content zero or bytes.

| FlowFile Processor | Black Box | 
Processors are what actually performs work.  In <<eip>> terms a processor is 
doing some combination of data Routing, Transformation, or mediation between
systems.  Processors have access to attributes of a given flow file and its 
content stream.  Processors can operate on zero or more FlowFiles in a given unit of work
and either commit that work or rollback.

| Connection | Bounded Buffer | 
Connections provide the actual linkage between processors.  These act as queues
and allow various processes to interact at differing rates.  These queues then 
can be prioritized dynamically and can have upper bounds on load which enables
back pressure.

| Flow Controller | Scheduler | 
The Flow Controller maintains the knowledge of how processes actually connect 
and manages the threads and allocations thereof which all processes use.  The
Flow Controller acts as the broker facilitating the exchange of FlowFiles 
between processors.

| Process Group | subnet | 
A Process Group is a specific set of processes and their connections which can
receive data via input ports and which can send data out via output ports.  In 
this manner process groups allow creation of entirely new components simply by
composition of other components.

|===========================

This design model, also similar to <<seda>>, provides many beneficial consequences which help NiFi 
to be a very effective platform for building powerful and scalable dataflows.
A few of these benefits include:

* Lends well to visual creation and management of directed graphs of processors
* Is inherently asynchronous which allows for very high throughput and natural buffering even as processing and flow rates fluctuate
* Provides a highly concurrent model without a developer having to worry about the typical complexities of concurrency
* Promotes the development of cohesive and loosely coupled components which can then be reused in other contexts and promotes testable units
* The resource constrained connections make critical functions such as back-pressure and pressure release very natural and intuitive
* Error handling becomes as natural as the happy-path rather than a coarse grained catch-all
* The points at which data enters and exits the system as well as how it flows through are well understood and easily tracked

NiFi Architecture
-----------------
image::nifi-arch.png["NiFi Architecture Diagram"]

NiFi executes within a JVM living within a host operating system.  The primary
components of NiFi then living within the JVM are as follows:

* Web Server
** The purpose of the web server is to host NiFi's HTTP-based command and control API.
* Flow Controller
** The flow controller is the brains of the operation. It provides threads for extensions to run on and manages their schedule of when they'll receive resources to execute.
* Extensions
** There are various types of extensions for NiFi which will be described in other documents.  But the key point here is that extensions operate/execute within the JVM.
* FlowFile Repository
** The FlowFile Repository is where NiFi keeps track of the state of what it knows about a given FlowFile that is presently active in the flow.  The implementation of the repository is pluggable.  The default approach is a persistent Write-Ahead Log that lives on a specified disk partition. 
* Content Repository
** The Content Repository is where the actual content bytes of a given FlowFile live.  The implementation of the repository is pluggable.  The default approach is a fairly simple mechanism which stores blocks of data in the file system.   More than one file system storage location can be specified so as to get different physical partitions engaged to reduce contention on any single volume.
* Provenance Repository
** The Provenance Repository is where all provenance event data is stored.  The repository construct is pluggable with the default implementation being to use  one or more physical disk volumes.  Within each location event data is indexed  and searchable.

NiFi is also able to operate within a cluster.

image::nifi-arch-cluster.png["NiFi Cluster Architecture Diagram"]

A NiFi cluster is comprised of one or more 'NiFi Nodes' (Node) controlled
by a single NiFi Cluster Manager (NCM).  The design of clustering is a simple
master/slave model where the NCM is the master and the Nodes are the slaves.
The NCM's reason for existence is to keep track of which Nodes are in the flow,
their status, and to replicate requests to modify or observe the 
flow.  Fundamentally then the NCM keeps the state of the cluster consistent.  
While the model is that of master and slave if the master dies the Nodes are all
instructed to continue operating as they were to ensure the data flow remains live.
The absence of the NCM simply means new nodes cannot come on-line and flow changes
cannot occur until the NCM is restored.

Performance Expections and Characteristics of NiFi
--------------------------------------------------
NiFi is designed to fully leverage the capabilities of the underlying host system
its is operating on.  This maximization of resources is particularly strong with
regard to CPU and disk.  Many more details will
be provided on best practices and configuration tips in the Administration Guide. 

- For IO:
The throughput or latency
one can expect to see will vary greatly on how the system is configured.  Given
that there are pluggable approaches to most of the major NiFi subsystems the
performance will vary greatly among them.  But, for something concrete and broadly
applicable lets consider the out of the box default implementations that are used.
These are all persistent with guaranteed delivery and do so using local disk.  So 
assume roughly 50 MB/s read/write rate on modest disks or RAID volumes 
within a modest server.  NiFi for a large class of data flows then be able to 
efficiently reach one hundred or more MB/s of throughput.  That is because linear growth
is expected for each physical parition and content repository added to NiFi up until
the rate of data tracking imposed on the FlowFile repository and provenance repository
starts to create bottlenecks.  We plan to provide a benchmarking/performance test template to 
include in the build which will allow users to easily test their system and 
to identify where bottlenecks are and at which point they might become a factor.  It 
should also make it easy for system administrators to make changes and to verity the impact.

- For CPU:
The FlowController acts as the engine dictating when a given processor will be
given a thread to execute.  Processors should be written to return the thread
as soon as they're done executing their task.  The FlowController can be given a 
configuration value indicating how many threads there should be for the various
thread pools it maintains.  What the ideal number to use will depend on the 
resources of the host system in terms of numbers of cores, whether that system is
running other services as well, and the nature of the processing in the flow.  For
typical IO heavy flows though it would be quite reasonable to set many dozens of threads
to be available if not more.

- For RAM:
NiFi lives within the JVM and is thus generally limited to the memory space it 
is afforded by the JVM.  Garbage collection of the JVM becomes a very important
factor to both restricting the total practical size the heap can be as well as
how well the application will run over time.  Processors built with no consideration
for memory contention will certainly causes garbage collection issues.  If FlowFile
attributes are used to store many large Strings and those then fill up the flow
that can create challenges as well.  There though NiFi will swap-out FlowFiles
sitting in queues that build up.  To do this it will write them out to disk.  This
is a very powerful feature for cases where a particular downstream consumer system 
is down for a period of time.  NiFi will safely swap out the FlowFile data from 
the heap and onto disk.  Once the flow starts moving again NiFi will gradually 
swap those items back in.  Within the framework great care is taken to be good
stewards of the JVM GC process and provided the same care is taken for all processors
and extensions in the flow then one can expect sustained efficient operation.

Dataflow Challenges : NiFi Features
-----------------------------------
* Systems fail
** Explanation: Networks fail, disks fail, software crashes, people make mistakes.
** Features: Fault-tolerance, buffering, durability, flow-specific QoS, data provenance, recovery/go back in time, visual command and control
* Data access exceeds capacity to consume
** Explanation: Sometimes a given data source can outpace some part of the processing or delivery chain - it only takes one weak-link to have an issue.
** Features: Prioritization, Back-pressure, congestion-avoidance, QoS (some things are critical and some are not)
* Boundary conditions are mere suggestions
** Explanation: You will get data that is too big, too small, too fast, too slow, corrupt, wrong, wrong format
** Features: flow-specific latency vs throughput tradeoffs, flow specific loss tolerance vs guaranteed delivery, extensible transformations
* What is noise one day becomes signal the next
** Explanation: Priorities of an organization change - rapidly.  Enabling new flows and changing existing ones must be fast.
** Features:  Dynamic prioritization of data.  Go back in time (rolling buffer of recorded history).  Real-time visual command and control.  Changes are immediate and fine-grained.
* Compliance and security
** Explanation: Laws and regulations change.  Business to business agreements change.  System to system and system to user interactions must be secure and trusted.
** Features: 2-Way SSL.  Pluggable authentication and authorization.  Data provenance.
* Continuous improvement occurs in production
** Explanation: It is often not possible to come even close to replicating production environments in the lab.
** Features: Flow-specific QoS.  Cheap copy-on-write.  Data provenance.  It is safe to tee a flow to an unreliable or non-production system.

# References
[bibliography]
- [[[eip]]] Gregor Hohpe. Enterprise Integration Patterns [online].  Retrieved: 27 Dec 2014, from: http://www.enterpriseintegrationpatterns.com/
- [[[soa]]] Wikipedia. Service Oriented Architecture [online]. Retrieved: 27 Dec 2014, from: http://en.wikipedia.org/wiki/Service-oriented_architecture
- [[[api]]] Eric Savitz.  Welcome to the API Economy [online].  Forbes.com. Retrieved: 27 Dec 2014, from: http://www.forbes.com/sites/ciocentral/2012/08/29/welcome-to-the-api-economy/
- [[[api2]]] Adam Duvander.  The rise of the API economy and consumer-led ecosystems [online]. thenextweb.com.  Retrieved: 27 Dec 2014, from: http://thenextweb.com/dd/2014/03/28/api-economy/
- [[[iot]]] Wikipedia. Internet of Things [online]. Retrieved: 27 Dec 2014, from: http://en.wikipedia.org/wiki/Internet_of_Things
- [[[bigdata]]] Wikipedia.  Big Data [online].  Retrieved: 27 Dec 2014, from: http://en.wikipedia.org/wiki/Big_data
- [[[fbp]]] Wikipedia.  Flow Based Programming [online].  Retrieved: 28 Dec 2014, from: http://en.wikipedia.org/wiki/Flow-based_programming#Concepts
- [[[seda]]] Matt Welsh.  Harvard.  SEDA: An Architecture for Highly Concurrent Server Applications [online].  Retrieved: 28 Dec 2014, from: http://www.eecs.harvard.edu/~mdw/proj/seda/